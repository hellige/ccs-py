{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCS 2.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(You'll need `pyrsistent`, `networkx` and `pygraphviz`. You'll also need graphviz.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be parsing and dealing with the actual CCS syntax, so we need to have access to the parser, and to understand a bit about how the AST represents rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "from ccs import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the syntax is \"primitive\" rules: property settings, additional context settings, imports, etc. Those are uninteresting, uniformly represented as leaf nodes of the AST. Normal rules with selectors (whether actually nested or not) are represented as nested rule-sets, each an optional selector and a list of nested rules, either primitive or further nested.\n",
    "\n",
    "Each selector is represented as an arbitrary boolean expression containing literals, conjunctions and disjunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "def parse_only(string):\n",
    "    return parser.Parser().parse(io.StringIO(string), '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parse_only(\"\"\"\n",
    "a, f b e, c { c d { x = y } e f { foobar = abc }}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parse_only(\"\"\"\n",
    "a f, b e, c { c d {}}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(parse_only(\"\"\"\n",
    "(a, b, c) (d, e, f) { g, h, i {}}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'll be handy in the below to be able to parse just the expression language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_expr(string):\n",
    "    return parser.Parser().parse_selector(io.StringIO(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parse_expr(\"(a, b, c) (d, e, f)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of matching rules to contexts amounts to testing formulae against truth-value assignments. But there are a few particularities which apply to our setting...\n",
    "\n",
    " - we want to evaluate rules under partial assignments as well as total assignemnts, identifying\n",
    "   rules which are not yet satisfied, but remain satisfiable by future assignments.\n",
    " - we're always monotonically extending partial assignments and then re-evaluating, so it seems \n",
    "   like a good idea to re-use the current state as a starting point for future evaluations.\n",
    " - many assignments will co-exist at the same time, and each may subsequently be used as the\n",
    "   starting point for future evaluations.\n",
    "   \n",
    "If we use an algorithm that just freshly evaluates all the rules against each assignment from scratch, there's not much more to do. If, however, we want to use an algorithm that performs work incrementally from the prior assignment, then we also want persistent data structures so that the old and new states can co-exist without copying.\n",
    "\n",
    "Here are some ideas..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brute force\n",
    "\n",
    " 1. Index rules by properties set in that context\n",
    " 2. Do nothing else until a property is requested\n",
    " 3. Find all rules setting that property\n",
    " 4. Evaluate each rule against the current assignment\n",
    " 5. Apply specificity logic, etc.\n",
    " \n",
    "This might be just fine, particularly if there are lots of different properties with only a few settings each. For situations where lots of properties are queried and set in the same context, there are some simple tricks that could speed things up. For instance, a given rule could be marked as satisfied (or not) by a given assignment so that future properties queried in the same context wouldn't need to reevaluate those rules. But in the case where are many, many settings of the same property in a large number of different rules, this seems unavoidably pretty bad.\n",
    "\n",
    "There maybe other use cases for which this approach wouldn't work as well. For instance, enumerating properties set in a given context, or enumerating all the possible values of a given key that could be added to reveal additional settings. Those are more rare, though, and it would be ok if they were more expensive, as long as they're tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAG/Rete\n",
    "\n",
    " 1. Use rules to build an immutable graph, with root nodes for each literal\n",
    " 2. As the context is extended with additional facts, propagate those facts through the graph,\n",
    "    activating child nodes as appropriate\n",
    " 3. When a node containing property settings is activated, add those settings to the properties\n",
    "    visible in the current context\n",
    " 4. Apply specificity logic, either separately or in step 3 when adding settings\n",
    " \n",
    "This is most similar to the implementation of CCS1. Given the requirements above, the node activation state must be kept separately from the main graph, and in a persistent data structure. Likewise the visible settings.\n",
    "\n",
    "Querying properties and enumerating properties are both trivial. The problem of enumerating key values remains tricky, but I've already implemented one variant and it's not too bad.\n",
    "\n",
    "There are lots of variants of this involving different approaches to building the dag. Since we're dealing with DNF terms, one possibility is to build a graph for disjunctions and one for conjunctions (or, equivalently, a single graph in two layers). The only thing remaining then is to optimize the structure of each of those graphs to amortize work done at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other matching approaches\n",
    "\n",
    "There are other matching approaches to indexing and matching boolean formulae. Perhaps one of these would be better, but it would need to support partial matching (to find satisfiable but not yet satisfied rules), and may need to support an incremental implementation using immutable data structures, depending on its overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCS 2\n",
    "\n",
    "For now, we use a DAG approach different from that in CCS1. Let's see how it works...\n",
    "\n",
    "In CCS1, we directly built dag nodes from this AST as we traversed it in a single pass. That won't work for CCS2, so we'll need to convert the AST to a collection of complete expressions, each with its list of primitive rules. In the process of doing this, we also normalize the expressions, a topic to which we now turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing and minimizing rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, our rules are represented as positive monotone formulae in DNF. This allows us to define specificity in a natural way, affords us a normal form suitable for diffing, and gives a uniform starting point for matching. Typical rules are actually closer to CNF, unfortunately, but with a couple of tricks, we can avoid an exponential blowup in the common cases.\n",
    "\n",
    "Because of the extra invariants, and because we'll be using these objects for matching, we represent them separately from the `Expr` type in the AST.\n",
    "\n",
    "A _clause_ is a set of literals, and a _formula_ a set of clauses. Formulae also have a slot for a set of _shared sub-clauses_, clauses known to be subclauses of at least two top-level clauses in the formula itself. The purpose of this will be clear later.\n",
    "\n",
    "Clauses and formulae are sorted first by size, then lexicographically. The purpose of sorting by size will be clear later as well. Other than that, we just need a bunch of boilerplate functionality. See the source file for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccs.formula import Clause, Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Formula([Clause({'a', 'b', 'c'}), Clause({'ab', 'e'}), Clause({'a', 'ab', 'c'})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any formula, we define a normal form which exists, is unique, and is equivalent to the original formula under the usual interpretation of boolean logic.\n",
    "\n",
    "Clauses are always normal, since all literals are positive. Formulae are normalized by removing any clause subsumed by any other. A clause $c$ is _subsumed_ by a clause $s$ if $s \\subseteq c$. This is the obvious $O(mn)$ algorithm. Our formulae are usually of size 1, so this is just fine.\n",
    "\n",
    "**TODO**: In fact, clauses could be further normalized, using the additional semantics of literals:\n",
    "\n",
    "    x.a x ... -> x.a ...\n",
    "    x.a x.!b ... -> x.a ...\n",
    "    x.!b x ... -> x.!b ...\n",
    "    x.a x.!a ... -> false (prune clause)\n",
    "    x.a x.b ... -> false (prune clasue)\n",
    "\n",
    "These are all relatively unlikely to be written by hand, though, so it may not be worth bothering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccs.formula import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form = Formula([Clause(['a', 'b']),\n",
    "                Clause(['b']),\n",
    "                Clause(['a']),\n",
    "                Clause(['c', 'd']),\n",
    "                Clause(['a', 'c', 'd']),\n",
    "                Clause(['c', 'd'])])\n",
    "form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to DNF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's much more convenient to specify rules as arbitrary boolean expressions (as is allowed in CCS1), particularly since in practice most things we want to express are closer to CNF than to DNF. We'll convert them to DNF, and can report an error if a formula expands beyond a fixed maximum size. We'll use a trick to handle the most common case of nested disjunctions, when we have a number of possible values for the same key. In a rule like this:\n",
    "\n",
    "    (key.val1, key.val2, key.val3) x y z\n",
    "\n",
    "which is by far the most common use of disjunction, there's really no need for us to expand it into three separate clauses. The specificity will be the same in any case, and we'll be matching literals by looking up the keys in a hash table anyway, in order to handle negative matches. So this doesn't add any complexity there either. The only thing we sacrifice is full normalization for purposes of diffing. Note that this is driven purely by hints in the input: we don't attempt to discover any new simplifications of this form.\n",
    "\n",
    "**TODO**: really verify this code and make sure this is really a good idea...\n",
    "\n",
    "We also notice and remember sharing introduced during DNF-conversion, so that it can be exploited later while building the DAG: whenever we're doing an `expand()` step, we gather up all the one-element children first and add a shared subclause containing those elements, and then expand all the multi-element children.\n",
    "\n",
    "`to_dnf()` converts an AST `Expr` to a normalized `Formula`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccs.ast import Expr, Op, Step\n",
    "from ccs.dag import Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccs.ast import flatten\n",
    "from ccs.dnf import to_dnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = to_dnf(parse_expr(\"a b, c d\"))\n",
    "print(s.shared)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = to_dnf(parse_expr(\"(a, b) (c, d)\"))\n",
    "print(s.shared)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = to_dnf(flatten(parse_expr(\"(a b) (c d)\")))\n",
    "print(s.shared)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = to_dnf(flatten(parse_expr(\"(a f (b, e)) (c d)\")))\n",
    "print(s.shared)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = to_dnf(flatten(parse_expr(\"a, (a f (b, e)) (c d)\")))\n",
    "print(s.shared)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = to_dnf(parse_expr(\"(a b) (c, d)\"))\n",
    "print(s.shared)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = to_dnf(flatten(parse_expr(\"(a, b), c d\")))\n",
    "print(s.shared)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = to_dnf(parse_expr(\"(a, b, c) (d, e, f) (g, h, i)\"))\n",
    "print(s.shared)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = to_dnf(flatten(parse_expr(\"a b (c q, d) (e r, f)\")))\n",
    "print(s.shared)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can allow the user to override a default limit to allow exploding expansions up to whatever size they really need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    to_dnf(parse_expr(\"(a, b, c) (d, e, f) (g, h, i)\"), limit=20)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example where normalization is important, we'll revisit this when we look at specificity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dnf(parse_expr(\"(a, b) (a, c)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing rulesets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, let's return to processing a whole ruleset. As mentioned above, we'll traverse the entire AST, combining each partial expression with its normalized parent to produce a new tree with each node having a complete normalized formula, its children, and any primitive rules defined at that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccs.rule_tree import RuleTreeNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'll be nice to be able to visualize these, so let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from IPython.display import Image\n",
    "    \n",
    "def draw_tree(tree):\n",
    "    G = nx.DiGraph()\n",
    "    G.graph['dpi'] = 60\n",
    "    def add_node(n):\n",
    "        l = n.label()\n",
    "        G.add_node(n, label=l if len(l) else \"-\", shape='box', style=\"filled\", fillcolor=n.color())\n",
    "        for c in n.children:\n",
    "            G.add_edge(n, c)\n",
    "            add_node(c)\n",
    "    add_node(tree)\n",
    "    return Image(nx.nx_agraph.to_agraph(G).draw(prog='dot',format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = parse_only(\"\"\"\n",
    "a, f b e, c {\n",
    "  c d {\n",
    "    x = y\n",
    "  }\n",
    "  e f {\n",
    "    foobar = abc\n",
    "  }\n",
    "}\n",
    "a, c, b e f : baz = quux\n",
    "\"\"\")\n",
    "root = RuleTreeNode()\n",
    "n.add_to(root)\n",
    "draw_tree(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there's no deduplication, pruning of empty nodes, etc. The structure just mirrors exactly the structure of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to see how the rule dag is structured! The dag will be rooted with a bunch of literals, which in actual practice consist of key/value-pattern matchable forms, but for now we'll just leave them opaque.\n",
    "\n",
    "Nodes consist of an operation (and/or), an activation tally count, their children and their property settings. The top-level dag contains an otherwise unused node for the root-level settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccs.dag import AndNode, LiteralMatcher, OrNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to be able to visualize these things too, so let's get that out of the way now. The second argument `t` will be used when we use the dags for matching below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO doesn't this visit the same nodes over and over??\n",
    "# t will be used later...\n",
    "def draw_dag(dag, t={}, active_only=False):\n",
    "    G = nx.DiGraph()\n",
    "    G.graph['dpi'] = 60\n",
    "    G.graph['rankdir'] = 'BT'\n",
    "    def add_nodes(p, ns):\n",
    "        for n in ns:\n",
    "            if active_only and n not in t:\n",
    "                continue\n",
    "                \n",
    "            if isinstance(n, OrNode):\n",
    "                label = 'V'\n",
    "                if n in t:\n",
    "                    color = 'palegreen'\n",
    "                else:\n",
    "                    color = 'lightblue'\n",
    "            else:\n",
    "                count = t.get(n, n.tally_count)\n",
    "                label = str(n.tally_count)\n",
    "                if count == 0:\n",
    "                    color = 'palegreen'\n",
    "                elif count != n.tally_count:\n",
    "                    color = 'mistyrose'\n",
    "                    label = \"{} / {}\".format(n.tally_count - count, label)\n",
    "                else:\n",
    "                    color = 'pink2'\n",
    "            style = 'filled'\n",
    "            if len(n.props): style += ', bold'\n",
    "            nodeid = f\"n{id(n)}\"\n",
    "            G.add_node(nodeid, label=label, style=style, fillcolor=color)\n",
    "            G.add_edge(p, nodeid)\n",
    "            add_nodes(nodeid, n.children)\n",
    "    for l, matcher in dag.children.items():\n",
    "        nodeid = f\"name_{id(l)}\"\n",
    "        G.add_node(nodeid, label=f'\"{l}\"')\n",
    "        if matcher.wildcard:\n",
    "            add_nodes(nodeid, [matcher.wildcard])\n",
    "        for v, nodes in matcher.positive_values.items():\n",
    "            nodeid2 = f\"value_{id(v)}\"\n",
    "            G.add_node(nodeid2, label=f'\"{v}\"', style='filled', fillcolor='lightyellow', shape='box')\n",
    "            G.add_edge(nodeid, nodeid2)\n",
    "            add_nodes(nodeid2, nodes)\n",
    "        # TODO handle negative values here\n",
    "    \n",
    "    return Image(nx.nx_agraph.to_agraph(G).draw(prog='dot',format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build a single dag containing both clauses and formulae, but the design is easier to understand if we regard them as two separate graphs. Consider the graph of clauses. The invariant is that a node represenging a clause $c$ needs to be reachable from exactly those root nodes representing literals $l \\in c$. It must be reachable via some path from each of those root nodes, and not reachable from any others.\n",
    "\n",
    "Subject to this, and without being too precise, the goal is to minimize the number of edges and also to minimize \n",
    "fan-out from any node. Intuitively, we want to minimize the duplication of work for overlapping formulae. Also, since we have no reason to believe that any particular fact will become true in the future, we also want to minimize the expected amount of work we have to do for any incremental addition of bindings to our assignment, deferring as much work as possible.\n",
    "\n",
    "To build the dag, we build the whole thing in two phases, first adding all clauses from smallest to largest, then adding all formulae likewise. For this reason, we require the complete set of formulae up-front. We use the greedy set-cover approximation algorithm for each phase. NB: This is where we rely on the fact that clauses and formulae are both ordered from smallest to largest.\n",
    "\n",
    "For reasons that will become clear below, we force every literal to build a disjunction node, but single-element clauses and formulae can simply reuse the node below them as their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccs.dag import build_dag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in place, we can build an entire rule tree. In the future, if we're really just going to immediately flatten the tree to a list of those nodes containing primitive rules, there's really no need to build the tree at all. But see below under **Finding additional sharing?** for a reason we may want to keep the trees around... So for now, we'll just flatten and sort it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_tree(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_dag(build_dag(filter(lambda n: len(n.props) + len(n.constraints), root)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to play around, we'll want a utility for going straight from expression strings to rule tree nodes that can be built into a dag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exprs(*strs):\n",
    "    return [RuleTreeNode(formula=to_dnf(parse_expr(str))) for str in strs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(x.formula for x in exprs('(a, b) (a, c)', 'a b c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(build_dag(exprs('a b c', 'a b', 'b c (e, f, g)', 'b d (e, f)', 'b')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few more little examples from my notes, just to show a few features of the way these dags are built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(build_dag(exprs('a b c', 'a b c')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(build_dag(exprs('a', 'b', 'c', 'a b', 'b c', 'a b c')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(build_dag(exprs('a', 'b', 'c', 'a b', 'b c', 'a b c', 'a c', 'a c d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(build_dag(exprs('a b c d', 'a b d', 'a c d', 'b d', 'a b')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(build_dag(exprs('b c (e, f, g)')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding additional sharing? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm will never create an intermediate node, so it'll only discover sharing among expressions that are actually present in the input. Compare the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(build_dag(exprs('a b c', 'b c d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(build_dag(exprs('a b c', 'b c d', 'b c')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But our input files themselves often contain strong hints about that, such as:\n",
    "\n",
    "    b c {\n",
    "      a {...}\n",
    "      d {...}\n",
    "    }\n",
    "    \n",
    "So we could get some mileage by exploiting these structures when we build the list of formulae."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll parse the above into a tree of formulae, all normalized:\n",
    "\n",
    "    b c\n",
    "     a b c\n",
    "     b c d\n",
    "        \n",
    "For each branch, we can check whether the parent is a subset of more than one of its children. If so, we add the parent formula to our set. Otherwise, we drop it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for\n",
    "\n",
    "    b {\n",
    "      c {\n",
    "        a {...}\n",
    "        d {...}\n",
    "      }\n",
    "    }\n",
    "    \n",
    "we find this tree:\n",
    "\n",
    "    b\n",
    "      b c\n",
    "       a b c\n",
    "       b c d\n",
    "\n",
    "We eliminate `b`, but retain `b c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = parse_only(\"\"\"\n",
    "b {\n",
    "  c {\n",
    "    a : x = 1\n",
    "    d : x = 2\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "root = RuleTreeNode()\n",
    "n.add_to(root)\n",
    "draw_tree(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(build_dag(exprs('a b c', 'b c d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(build_dag(exprs('b c', 'a b c', 'b c d')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For\n",
    "\n",
    "    (a, b) {\n",
    "      a {...}\n",
    "      c {...}\n",
    "    }\n",
    "    \n",
    "we find\n",
    "\n",
    "    ab d\n",
    "     a d\n",
    "     ab c d\n",
    "\n",
    "and since `ab d` is a subset of only one of its children, we eliminate it, saving one node.\n",
    "\n",
    "Note that there are cases where this still builds unwanted intermediate nodes. For instance:\n",
    "\n",
    "    a b c {...}\n",
    "    a b {\n",
    "      c d {...}\n",
    "      c e {...}\n",
    "    }\n",
    "\n",
    "The `a b` node will be included in this case, optimistically hoping that it'll be used for `a b c d` and `a b c e`, but in fact both of those will take advantage of `a b c`, leaving `a b` with no other children. Truly orphaned nodes and nodes with only one child and no settings/assertions like that could just be ignored, they could be cleaned out at the end, or maybe the above subset heuristic could be improved to detect them sooner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = parse_only(\"\"\"\n",
    "a b c {x = 1}\n",
    "a b {\n",
    "  c d {x = 2}\n",
    "  c e {x = 3}\n",
    "}\n",
    "\"\"\")\n",
    "root = RuleTreeNode()\n",
    "n.add_to(root)\n",
    "draw_tree(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(build_dag(exprs('a b c', 'a b c d', 'a b c e')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(build_dag(exprs('a b', 'a b c', 'a b c e', 'a b c e')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Question:** Does this section up to here still make sense when we're building DNF rather than CNF?_\n",
    "\n",
    "In any case, this is a possible extension for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example that shows the value of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(build_dag(exprs('a b c d e (f, g, h, i, j)')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, let's talk about matching against these dags. This part is simple. We just start at the correct root for the newly added literal fact, and activate whichever nodes we reach, decrementing the tally count of each. We activate a node's children only when its tally count reaches *exactly* zero. This prevents duplicate activations in the case of disjunctions. (It would also allow an easy way to poison nodes, if ever needed.) We also need to be careful not to allow duplicate literals, but forcing each single-element clause to build a one-input disjunction node accomplishes that as well.\n",
    "\n",
    "Finally, we keep the tally counts in a persistent map so that we can fork them as discussed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as specificity is concerned, there are three types of nodes:\n",
    "\n",
    " 1. disjunction nodes representing literals\n",
    " 2. conjunction nodes\n",
    " 3. true disjunction nodes representing formulae\n",
    "\n",
    "In the case of 1, activation specificity is constant, and the node only needs to be activated once.\n",
    "\n",
    "In the case of 2, with a bit of care when building literal nodes, we can calculate the exact specificity as we build the dag and store it in the node. So the node only needs to be activated at most once, and specificity is known.\n",
    "\n",
    "In the case of 3, the activation specificity is always the specificity propagated from the activating node, and activation occurs not only the first time, but whenever to newly propagated specificity is greater than any previous activation specificity.\n",
    "\n",
    "This suggests that we need two node types, with distinct activation policies, one for cases 1 and 2, and one for case 3. The tally state for the first node type is just the current activation count, and the tally state for the second node type is the last activation specificity, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccs.search_state import Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, there will be some extra wrinkles pertaining to the more complex real-world structure of literals, property settings, specificities, rule-activated fact augmentations, and so on. But it's all fairly straightforward after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = build_dag(exprs('a b c', 'a b', 'b c (e,f,g)', 'b d (e,f)', 'b'))\n",
    "root = Context(dag)\n",
    "c = root.augment('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the state of a graph in context as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(dag, c.tallies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(dag, c.augment('a').tallies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(dag, c.augment('e').tallies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def augmentAll(c, ls):\n",
    "    return reduce(lambda c, l: c.augment(l), ls, c)\n",
    "draw_dag(dag, augmentAll(root, 'bdcg').tallies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the rules for these examples were\n",
    "\n",
    "    exprs('a b c', 'a b', 'b c (e,f,g)', 'b d (e,f)', 'b')\n",
    "\n",
    "so those activations are exactly as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows the necessity of tracking dirty literals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dag(dag, augmentAll(root, 'aa').tallies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this example shows why we build set-valued literal nodes as tally-1 conjunctions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = parse_only(\"\"\"\n",
    "(foo.a, foo.b, foo.c) a b : x = 123\n",
    "\"\"\")\n",
    "rule_tree = RuleTreeNode()\n",
    "n.add_to(rule_tree)\n",
    "dag = build_dag(filter(lambda n: len(n.props) + len(n.constraints), rule_tree))\n",
    "root = Context(dag)\n",
    "draw_dag(dag, root.augment('foo', 'a').tallies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than forcing an indirection of every literal through a disjunction node, we could also just track dirty literals explicitly. But the current approach is probably better, since we need a node on which to hang settings and constraints in any case. And it's more consistent and simpler, even if it makes the pictures a little uglier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrsistent\n",
    "\n",
    "n = parse_only(\"\"\"\n",
    "a, f b e, c {\n",
    "  c d {\n",
    "    x = y\n",
    "  }\n",
    "  e f {\n",
    "    foobar = abc\n",
    "  }\n",
    "}\n",
    "a, c, b e f : baz = quux\n",
    "\"\"\")\n",
    "rule_tree = RuleTreeNode()\n",
    "n.add_to(rule_tree)\n",
    "dag = build_dag(filter(lambda n: len(n.props) + len(n.constraints), rule_tree))\n",
    "root = Context(dag)\n",
    "\n",
    "def check(keys):\n",
    "    ctx = augmentAll(root, keys)\n",
    "    print(pyrsistent.thaw(ctx.props))\n",
    "    return draw_dag(dag, ctx.tallies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check('aef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check('cd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check('feb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick exercise of CCS1 override functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = parse_only(\"\"\"\n",
    "env.dev day.today {\n",
    "   foo = right\n",
    "   bar = wrong\n",
    "}\n",
    "env.dev {\n",
    "  foo = wrong\n",
    "  @override bar = right\n",
    "}\n",
    "\"\"\")\n",
    "rule_tree = RuleTreeNode()\n",
    "n.add_to(rule_tree)\n",
    "dag = build_dag(filter(lambda n: len(n.props) + len(n.constraints), rule_tree))\n",
    "root = Context(dag)\n",
    "\n",
    "ctx = root.augment('day', 'today').augment('env', 'dev')\n",
    "assert list(ctx.props['foo'].values)[0].value == 'right'\n",
    "assert list(ctx.props['bar'].values)[0].value == 'right'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = parser.Parser().parse(io.StringIO(\"\"\"\n",
    "(a.x, a.y, a.z) b c d : foo = bar\n",
    "a c : foo = baz\n",
    "\"\"\"), '-')\n",
    "tmp2 = RuleTreeNode()\n",
    "tmp1.add_to(tmp2)\n",
    "tmp3 = build_dag(filter(lambda n: len(n.props)+len(n.constraints), tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "class ImportResolver:\n",
    "    def __init__(self, basedir):\n",
    "        self.basedir = basedir\n",
    "        \n",
    "    def resolve(self, location):\n",
    "        if not os.path.isabs(location):\n",
    "            location = os.path.join(self.basedir, location)\n",
    "        if os.path.isfile(location):\n",
    "            return open(location)\n",
    "        raise ValueError(f\"No such file: '{location}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolver = ImportResolver('tmp')\n",
    "\n",
    "%time tmp1 = parser.Parser().parse_ccs_stream(resolver.resolve('main.ccs'), 'main.ccs', resolver, [])\n",
    "tmp2 = RuleTreeNode()\n",
    "%time tmp1.add_to(tmp2)\n",
    "%time tmp3 = build_dag(filter(lambda n: len(n.props)+len(n.constraints), tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw_tree(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw_dag(tmp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tmp2.stats())\n",
    "tmp3.stats().dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ctx_from_file(dag, fname, *, poison=False):\n",
    "    with open(fname) as f:\n",
    "        path = f.read()\n",
    "    root = Context(dag, poisoned=pyrsistent.s() if poison else None)\n",
    "\n",
    "    import re\n",
    "    steps = re.findall(r\"([A-Za-z0-9_-]+(\\.[A-Za-z0-9_-]+)?)\", path)\n",
    "    ctx = root\n",
    "    for step in steps:\n",
    "        pair = step[0].split('.')\n",
    "        ctx = ctx.augment(pair[0], pair[1] if len(pair) > 1 else None)\n",
    "        \n",
    "    return ctx\n",
    "        \n",
    "ctx = ctx_from_file(tmp3, 'example_ctx1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyrsistent.thaw(ctx.props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw_dag(tmp3, ctx.tallies, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical dumping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's the idea behind this one... we want to dump all the property \n",
    "# settings with the normalized formula for each. we want to be able to \n",
    "# exclude settings which can never match in the current context. this\n",
    "# isn't done yet, but it means we'll want to operate on the dag plus\n",
    "# tally state, rather than on the rule tree, which would otherwise probably\n",
    "# be simpler.\n",
    "\n",
    "# we top sort the dag depth-first starting at each literal node. as we \n",
    "# visit each literal, we propagate its literal into the expression\n",
    "# being built for the destination node. then all that remains is to scan\n",
    "# the nodes in top sort order, propagating each node's clause/formula\n",
    "# onward to its children and dumping any properties present in the node\n",
    "# itself. we must do this in two phases because the literal sets in the\n",
    "# initial nodes of the top sort aren't fully populated until the initial\n",
    "# scan of the literals is finished (and same with the other dag nodes, in\n",
    "# fact).\n",
    "\n",
    "# finally we sort first by property name, then by formula, and print the\n",
    "# results.\n",
    "\n",
    "def top_sort(dag):\n",
    "    visited = set()\n",
    "    result = []\n",
    "    \n",
    "    def visit(n):\n",
    "        if n in visited:\n",
    "            return\n",
    "        for m in n.children:\n",
    "            visit(m)\n",
    "        visited.add(n)\n",
    "        result.append(n)\n",
    "\n",
    "    node_forms = {}\n",
    "    \n",
    "    def visit_literal_node(node, name, value=None):\n",
    "        # at this point, we know all the nodes are going to be\n",
    "        # AndNodes, but we also know that in actual fact they correspond\n",
    "        # to disjunctions of literals, so we do something a bit special\n",
    "        # to build the clause:\n",
    "        assert isinstance(node, AndNode)\n",
    "        \n",
    "        # TODO this won't really work right for disjunctions of a wildcard\n",
    "        # plus actual values (as in '(a, a.x, a.y) : foo = bar'), but i'm\n",
    "        # pretty sure that's already broken other places as well. anyway,\n",
    "        # add a test and fix it everywhere!\n",
    "        if node in node_forms:\n",
    "            values = node_forms[node].first().values\n",
    "        else:\n",
    "            values = set()\n",
    "        to_add = {value} if value else set()\n",
    "        node_forms[node] = Clause([Key(name, values | to_add)])\n",
    "        visit(node)\n",
    "        \n",
    "    for l, matcher in dag.children.items():\n",
    "        if matcher.wildcard:\n",
    "            visit_literal_node(matcher.wildcard, l)\n",
    "        for v, nodes in matcher.positive_values.items():\n",
    "            for node in nodes:\n",
    "                visit_literal_node(node, l, v)\n",
    "        # TODO handle negative values here        \n",
    "        \n",
    "    return reversed(result), node_forms\n",
    "\n",
    "\n",
    "# TODO this is terrible and wants a cleanup!\n",
    "def combine(f1, f2):\n",
    "    if isinstance(f1, Clause) and isinstance(f2, Clause):\n",
    "        return f1.union(f2)\n",
    "    if isinstance(f1, Formula) and isinstance(f2, Formula):\n",
    "        return Formula(f1.clauses.union(f2.clauses))\n",
    "    if isinstance(f1, Clause) and isinstance(f2, Formula):\n",
    "        return Formula(f2.clauses | {f1})\n",
    "    assert False, f\"what are you trying to do? {type(f1)} {type(f2)}\"\n",
    "\n",
    "    \n",
    "import sys    \n",
    "def dump_dag(ctx, *, out=sys.stdout):\n",
    "    dag = ctx.dag\n",
    "    poisoned = ctx.poisoned or pyrsistent.s()\n",
    "    nodes, node_forms = top_sort(dag)\n",
    "    \n",
    "    results = []\n",
    "    for node in nodes:\n",
    "        # TODO is this the correct place to bail out here? or only when we add the props to result? think hard about this!\n",
    "        if node in poisoned:\n",
    "            continue\n",
    "        form = node_forms[node]\n",
    "        for prop in node.props:\n",
    "            # TODO this is terrible, find a better way to do it. in general,\n",
    "            # wouldn't it be easier to just store the normalized formula\n",
    "            # in each node?? how much memory could that possibly really waste?\n",
    "            prop_form = Formula([form]) if isinstance(form, Clause) else form\n",
    "            results.append((prop_form, prop)) # TODO include origin!\n",
    "        # TODO also handle constraints!\n",
    "        for child in node.children:\n",
    "            if child in node_forms:\n",
    "                child_form = node_forms[child]\n",
    "            else:\n",
    "                child_form = Clause([]) if isinstance(child, AndNode) else Formula([])\n",
    "            node_forms[child] = combine(form, child_form)\n",
    "\n",
    "    def sort_key(result):\n",
    "        return (result[1][0], result[0])\n",
    "        \n",
    "    for result in sorted(results, key=sort_key):\n",
    "        name = result[1][0]\n",
    "        prop = result[1][1]\n",
    "        ovd = \"@override \" if prop.override_level > 0 else \"\"\n",
    "        print(f\"{result[0]} : {ovd}{name} = '{prop.value}' // {prop.origin}\", file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('tmp/asdf6.ccs', 'w') as out:\n",
    "    dump_dag(ctx_from_file(tmp3, 'example_ctx3.txt', poison=True), out=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be diffed as in:\n",
    "\n",
    "https://stackoverflow.com/questions/44019/an-easy-way-to-diff-log-files-ignoring-the-time-stamps\n",
    "\n",
    "\n",
    "Here's a side-by-side diff, ignoring the property origins, the works even with Mac OS's crappy sed. (There's probably an even better way to handle the newline insertion on MacOS sed, but this is good enough to prove the point...)\n",
    "\n",
    "    diff -y -I '^  //' <(sed  -E 's:^(.*)//:\\1#  //:' foo.dump | tr \\# \\\\n) <(sed  -E 's:^(.*)//:\\1#  //:' bar.dump | tr \\# \\\\n)\n",
    "\n",
    "And meld can do it easily with one of its default 'file filters'!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - override/underride!\n",
    " - conflict detection/resolution by property number\n",
    " - negative matches!\n",
    " - unset!\n",
    " - clean up, port and update tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    " - conflicts should be reported when queried, not when loaded, probably even in case of two settings in the exact same dag node?\n",
    " - add test for multiple values for same key augmented at the same time.\n",
    " - should we allow something like `foo > foo.bar` or is that a disallowed duplicate value?\n",
    " - we *should* allow multiple assertions of the same fact, like `foo.bar > foo.bar`. This allows us to add known implications to the files themselves, which can help simplify the output of dump/diff by pruning things that are known to be irrelevant, but not necessarily yet present in the input context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
